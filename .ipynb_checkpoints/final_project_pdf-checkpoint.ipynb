{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  By some estimates, 90% of the world's languages are going to die by the end of the 21st century.[^1]  \n",
    "  Other estimations put this number closer to 50% (at a rate of losing a language every 3 months)[^2],   \n",
    "    \n",
    "\tbut regardless of how much the ending result is the same: we're losing the histories of people at a rapid rate. Because many small cultures rely on oral or written scriptures to tell the story of their people, we risk losing such an integral part of our human history through the loss of language.\n",
    "\n",
    "  And studies have shown that linguistic fractionalization benefits benefits from the worst of globalization: \"countries that have a high degree of fractionalization have less income redistribution, while capitalist countries that have a low degree of fractionalization have more income distribution.\"[^3]<u>Through this project I want to explore the ideas of how globalization, in some instances, are erasing divergences amongst small populations in favor of the majority: specifically if globalization is diminishing fractionalization and encouraging the erasure of regional linguistic variations.</u>\n",
    "\n",
    "  In the following report I will outline my attempt at answering this question by utilizing the skills I developed during my PPOL 564 class. This will include an exploration of current literature surrounding the topic; my data collection and clean-up; data visualizations; as well as a machine learning component to test the predictive ability of the globalization index. All throughout,I will also highlight the problems I ran into and well as discussing my thought process for why I did what I did. \n",
    "\n",
    "[^1]: Hale, Ken. “Endangered Languages: On Endangered Languages and the Safeguarding of Diversity.” Language, vol. 68, no. 1, 1992, pp. 1–42. DOI.org (Crossref), doi:10.1353/lan.1992.0052.\n",
    "\n",
    "[^2]: New Estimates on the Rate of Global Language Loss - The Rosetta Project. https://rosettaproject.org/blog/02013/mar/28/new-estimates-on-rate-of-language-loss/.\n",
    "\n",
    "[^3]: Sturm, Jan-Egbert, and Jakob de Haan. “Income Inequality, Capitalism and Ethno-Linguistic Fractionalization.” SSRN Electronic Journal, 2014. DOI.org (Crossref), doi:10.2139/ssrn.2550652."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement and Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  When talking about current ethnolinguistic fractionalization (ELF) research, much of it frames the question in terms of the cost and benefit of diversity. The “cost” of diversity here is through the rise of conflict—whether that be through racism, homophobia, sexism the end results is social unrest and discriminatory attitudes. These have direct negative policy effects as policymakers are forced to address those issues rather than utilizing their already limited resources to create policies that directly effect the productivity of country.[^4] The benefits are also framed in terms of productivity: a diversity of abilities, experiences, and cultures leads to innovating thinking and creative solution making.\n",
    "\t\n",
    "  And when following this purely cost/benefit framework of ELF, there appears to be a negative relationship between ELF and various economic factors such as economic growth[^5], income inequality (addressed above), unemployment rate[^6], and institutional quality[^7]. \n",
    "\n",
    "  But what I find more curious, and there doesn’t appear to be that much literature on this question, is about the opposite effect—how have these economic factors, or more broadly globalization, effected ELF? My hypothesis is that there is a strong correlation between globalization factors and ELF and this conceptually makes sense. As people move to bigger cities, get better jobs, learn English (or the nation’s majority language) the more likely they are to forget their mothertongue or utilize it less frequently. \n",
    "  \n",
    "**Main Project Question** What are the effects of globalization on linguistic fractionalization? Are there globalization measures that are good predictors for linguistic fractionalization? \n",
    "\n",
    "[^4]: Alesina, Alberto, et al. *Fractionalization*. w9411, National Bureau of Economic Research, Jan. 2003, p. w9411. DOI.org (Crossref), doi:10.3386/w9411.\n",
    "\n",
    "[^5]: Alesina, Alberto, *et al.* \n",
    "\n",
    "[^6]: Feldmann, Horst. “Ethnic Fractionalization and Unemployment.” *Economics Letters*, vol. 117, no. 1, Oct. 2012, pp. 192–95. ScienceDirect, doi:10.1016/j.econlet.2012.04.034.\n",
    "\n",
    "[^7]: Alesina, Alberto, and Ekaterina Zhuravskaya. 2011. \"Segregation and the Quality of Government in a Cross Section of Countries.\" *American Economic Review*, 101 (5): 1872-1911. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Although my original project proposal detailed using 6 different data sources I ended up using 4 different sources instead.  \n",
    "\n",
    "* Linguistic fractionalization in the year 2000 - Alberto Alesina, Arnaud Devleeschauwer, William Easterly, Sergio Kurlat & Romain Wacziarg [^8]\n",
    "    * Variable of interest: Linguistic Fractionalization— % of the population\n",
    "* Ethnolinguistic Fractionalization (ELF) Indices, 1961 and 1985 - Philip G. Roeder[^9]\n",
    "  * Variable of interest: Linguistic Fractionalization— % of the population\n",
    "* Education Attainment by Age Group – Barro-Lee [^10]\n",
    "  * Variable of interest: Completion of Secondary School— % of the female population aged 15 and older\n",
    "* The World Bank Dataset\n",
    "  * Variables of interest:\n",
    "    * Infant Mortality Rate— the number of infants dying before reaching one year of age, per 1,000 live births in a given year [^11]\n",
    "    * Migrant Patterns— the total number of immigrants minus the annual number of emigrants) [^12]\n",
    "    * Urban Population— % of the population living in urban areas [^13]\n",
    "    * Rural Population— % of the population living in rural areas [^14]\n",
    "    * Cereal Yield— kilograms per hectare of harvested land [^15]\n",
    "    * GDP— annual percentage growth rate of GDP [^16]\n",
    "\n",
    "For the purposes of this project I used both ELF measures as my dependent variable and Female Educational Attainment, Infant Mortality Rate (IMR), Migrant Patterns, Urban Populations, Rural Populations, Cereal Yields, and GDP as my independent variables. To quickly touch on why I chose the independent variables I did: IMR and Female Educational Attainment are great indicator of the overall health of a society[^17][^18]; migration increases the world GDP [^19]; and there is a positive relationship between urbanization and globalization.[^20] I also threw in Cereal Yields as a random variable just out of curiosity. \n",
    "\n",
    "In terms of data clean up, given that all of my data came in the form of excel files, a lot of effort was spent in terms of getting it Python ready. I will admit there was a visual aspect to this process where I would pull up the excel file and visually make note of the columns I was interested in. But because I was mostly dealing with excel file this also meant that I had to convert “excel grammar” into Python. That is to say, the column headers would sometimes be in a row of the actual dataset; if cells were vertically merged on Excel this would usually output into two extra *NaN* cells on top of the cell with the actual value; researcher would sometimes just leave the country column blank because the country name was already reference at the top of the sheet, etc.\n",
    "\n",
    "The generic packages used for this project include: `Pandas`[^21],  and `Missngno`[^22]. `Pandas` was used for data manipulation and analysis whereas `Missingno` was used to visualize missing data. Other packages were also used but they will be mentioned in the section that they show up.\n",
    "\n",
    "[^8]: Alesina, Alberto, et al. *Fractionalization*. \n",
    "\n",
    "[^9]: Ethnolinguistic Fractionalization. http://pages.ucsd.edu/~proeder/elf.htm.\n",
    "\n",
    "[^10]: Barro-Lee Educational Attainment Data. http://www.barrolee.com/. \n",
    "\n",
    "[^11]: *Mortality Rate, Infant (per 1,000 Live Births) | Data.* The World Bank. https://data.worldbank.org/indicator/SP.DYN.IMRT.IN. \n",
    "\n",
    "[^12]: *Net Migration | Data.* The World Bank. https://data.worldbank.org/indicator/SM.POP.NETM.\n",
    "\n",
    "[^13]: *Urban Population (% of Total Population) | Data.* The World Bank. https://data.worldbank.org/indicator/SP.URB.TOTL.IN.ZS. \n",
    "\n",
    "[^14]: *Rural Population (% of Total Population) | Data.* The World Bank. https://data.worldbank.org/indicator/SP.RUR.TOTL.ZS.\n",
    "\n",
    "[^15]: *Cereal Yield (Kg per Hectare) | Data.* The World Bank. https://data.worldbank.org/indicator/AG.YLD.CREL.KG. \n",
    "\n",
    "[^16]: *GDP Growth (Annual %) | Data.* The World Bank, https://data.worldbank.org/indicator/NY.GDP.MKTP.KD.ZG. \n",
    "\n",
    "[^17]: *Infant Mortality | Maternal and Infant Health | Reproductive Health | CDC.* 10 Sept. 2020, https://www.cdc.gov/reproductivehealth/maternalinfanthealth/infantmortality.htm.\n",
    "\n",
    "[^18]: “Socioeconomic Impact of Female Education.” *Wikipedia*, 12 Dec. 2020. *Wikipedia*, https://en.wikipedia.org/w/index.php?title=Socioeconomic_impact_of_female_education&oldid=993696908.\n",
    "\n",
    "[^19]: INTERNATIONAL MONETARY FUND. *WORLD ECONOMIC OUTLOOK, APRIL 2020.* INTL MONETARY FUND, 2020.\n",
    "\n",
    "[^20]: Richard, Florida. “A New Typology of Global Cities.” What Globalization and Urbanization Mean for Global Cities, Bloomberg, 6 Oct. 2016, https://www.bloomberg.com/news/articles/2016-10-06/what-globalization-and-urbanization-mean-for-cities.\n",
    "\n",
    "[^21]: Jeff Reback, et. al. (2020, March 18). pandas-dev/pandas: Pandas 1.0.3 (Version v1.0.3). Zenodo. http://doi.org/10.5281/zenodo.3715232\n",
    "\n",
    "[^22]: Aleksey Bilogur, samuelbr, Volker Beutner, Zhian N. Kamvar, Harry Mavroforakis, & Ben Everson. (2018, February 27). ResidentMario/missingno 0.4.0 (Version 0.4.0). Zenodo. http://doi.org/10.5281/zenodo.1184723"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Management/Investigation\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno as miss\n",
    "from plotnine import *\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import country_converter as coco\n",
    "\n",
    "# For pre-processing data\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# For splits and CV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold # Cross validation \n",
    "from sklearn.model_selection import cross_validate # Cross validation \n",
    "from sklearn.model_selection import GridSearchCV # Cross validation + param. tuning.\n",
    "\n",
    "# Machine learning methods\n",
    "from sklearn.naive_bayes import GaussianNB as NB\n",
    "from sklearn.linear_model import LinearRegression as LM\n",
    "from sklearn.neighbors import KNeighborsRegressor as KNN\n",
    "from sklearn.tree import DecisionTreeRegressor as DT\n",
    "from sklearn.ensemble import RandomForestRegressor as RF\n",
    "\n",
    "# For evaluating our model's performance\n",
    "import sklearn.metrics as m\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Pipeline to combine modeling elements\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Tag Management\n",
    "from traitlets.config import Config\n",
    "import nbformat as nbf\n",
    "from nbconvert.exporters import PDFExporter\n",
    "from nbconvert.preprocessors import TagRemovePreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Config()\n",
    "\n",
    "# Configure our tag removal\n",
    "c.TagRemovePreprocessor.remove_cell_tags = (\"remove_cell\",)\n",
    "c.TagRemovePreprocessor.remove_all_outputs_tags = ('remove_output',)\n",
    "c.TagRemovePreprocessor.remove_input_tags = ('remove_input',)\n",
    "c.TagRemovePreprocessor.enabled=True\n",
    "\n",
    "# # Configure and run out exporter\n",
    "# c.PDFExporter.preprocessors = [\"TagRemovePreprocessor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imported 2001 fractionalization dataset & dropping 5 rows\n",
    "frac_2001 = pd.read_excel(r'Datasets/2003_fractionalization.xls').drop([0,1,217,218,219])\n",
    "\n",
    "#Renamed the columns\n",
    "frac_2001.columns = ['Temp_Country', 'Source (Ethnicity Data)', 'Date (Ethnicity Data)', 'Ethnic', 'ELF', 'Religion']\n",
    "\n",
    "#Replaced rows with value '.' to 'NaN'\n",
    "frac_2001 = frac_2001.replace('.', 'NaN')\n",
    "\n",
    "#Dropped specific columns\n",
    "frac_2001 = frac_2001.drop(columns = ['Source (Ethnicity Data)', 'Date (Ethnicity Data)', 'Ethnic', 'Religion']).reset_index(drop = True)\n",
    "\n",
    "#Inserted a row \n",
    "frac_2001.insert(1, \"Year\", \"2001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imported 1961 and 1985 fractionalization dataset\n",
    "frac_6185 = pd.read_excel(r'Datasets/fractionalization_2000.xls').drop([0]).reset_index(drop = True)\n",
    "\n",
    "#Dropped more columns \n",
    "frac_6185 = frac_6185.drop(columns = [1, 3, 4, 5, 7, 8, 10, 11, 12])\n",
    "\n",
    "#Renamed the columns\n",
    "frac_6185.columns = ['Temp_Country', '1961', '1985']\n",
    "\n",
    "#Melted the dataframe\n",
    "frac_6185 = pd.melt(frac_6185, id_vars=['Temp_Country'])\n",
    "\n",
    "#Renamed the columns\n",
    "frac_6185.columns = ['Temp_Country', 'Year', 'ELF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imported the World Bank Infant Mortality Dataset\n",
    "IMD = pd.read_excel(r'Datasets/IMR.xls')\n",
    "\n",
    "#Imported the World Bank Rural Dataset\n",
    "rural = pd.read_excel(r'Datasets/rural.xls')\n",
    "\n",
    "#Imported the World Bank Urban Dataset\n",
    "urban = pd.read_excel(r'Datasets/urban.xls')\n",
    "\n",
    "#Imported the World Bank Migration Dataset\n",
    "migration = pd.read_excel(r'Datasets/migration.xls')\n",
    "\n",
    "#Imported the World Bank GDP Dataset\n",
    "GDP = pd.read_excel(r'Datasets/GDP.xls')\n",
    "\n",
    "#Imported the World Bank Cereal Dataset\n",
    "cereal = pd.read_excel(r'Datasets/cereal.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created a world bank function that converts all the world bank data into a similar format \n",
    "def world_bank(df):\n",
    "    '''\n",
    "    Function that takes a World Bank dataframe and adds the appropriate column headings for the pertinent columns.\n",
    "    This function also drops any columns that are not pertinent to the research questions.\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): unedited World Bank dataframe\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: edited dataframe with the correct columns\n",
    "    '''\n",
    "    names = []\n",
    "    for x in df.iloc[2]: #Created a for loop that created a list of columns names from the 3rd row and change the datatype as appropriate\n",
    "        if isinstance(x, str):\n",
    "            names.append(x)\n",
    "        elif isinstance(x, float):\n",
    "            y = x.astype(int)\n",
    "            names.append(y)\n",
    "    df.columns = names\n",
    "    notdrop = ['Country Name', 1961, 1985, 2001] #Created a list of the column names I am interested in\n",
    "    for x in df.columns: #Dropped all columns that were not in 'notdrop' list\n",
    "        if x not in notdrop:\n",
    "            df = df.drop(columns = [x])\n",
    "    df.columns = ['Temp_Country', '1961', '1985', '2001'] #Renamed the columns\n",
    "    df = df.drop([0,1,2]) #Dropped rows\n",
    "    df = pd.melt(df, id_vars=['Temp_Country']) #Melted the table into the format I was interested in\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created a world bank function that converts all the world bank data into a similar format \n",
    "def wb_migration(df_2):\n",
    "    '''\n",
    "    Function that takes a World Bank Migration dataframe and adds the appropriate column headings for the pertinent columns.\n",
    "    This function also drops any columns that are not pertinent to the research questions. This function is different \n",
    "    than the above function as the Migration dataset has measurements happening every 5 years starting from 1962. \n",
    "    As such, I am using data from 1962, 1987 and 2002 to be representative for the migration patterns in 1961, 1985, \n",
    "    and 2001.\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): unedited World Bank Migration dataframe\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: edited Migration dataframe with the correct columns\n",
    "    '''\n",
    "    names_2 = []\n",
    "    for z in df_2.iloc[2]: #Created a for loop that created a list of columns names from the 3rd row and change the datatype as appropriate\n",
    "        if isinstance(z, str):\n",
    "            names_2.append(z)\n",
    "        elif isinstance(z, float):\n",
    "            w = z.astype(int)\n",
    "            names_2.append(w)\n",
    "    df_2.columns = names_2\n",
    "    notdrop_2 = ['Country Name', 1962, 1987, 2002] #Created a list of the column names I am interested in\n",
    "    for z in df_2.columns: #Dropped all columns that were not in 'notdrop' list\n",
    "        if z not in notdrop_2:\n",
    "            df_2 = df_2.drop(columns = [z])\n",
    "    df_2.columns = ['Temp_Country', '1961', '1985', '2001'] #Renamed the columns\n",
    "    df_2 = df_2.drop([0,1,2]) #Dropped rows\n",
    "    df_2 = pd.melt(df_2, id_vars=['Temp_Country']) #Melted the table into the format I was interested in\n",
    "    return df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ran the world_bank function for the World Bank datasets and renamed the columns as appropriate\n",
    "IMD = world_bank(IMD)\n",
    "IMD.columns = ['Temp_Country', 'Year', 'Infant']\n",
    "\n",
    "rural = world_bank(rural)\n",
    "rural.columns = ['Temp_Country', 'Year', 'Rural_%']\n",
    "\n",
    "urban = world_bank(urban)\n",
    "urban.columns = ['Temp_Country', 'Year', 'Urban_%']\n",
    "\n",
    "GDP = world_bank(GDP)\n",
    "GDP.columns = ['Temp_Country', 'Year', 'GDP']\n",
    "\n",
    "cereal = world_bank(cereal)\n",
    "cereal.columns = ['Temp_Country', 'Year', 'Cereal_Yield']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ran the migration function for the World Bank migration dataset and renamed the columns as appropriate\n",
    "migration = wb_migration(migration)\n",
    "migration.columns = ['Temp_Country', 'Year', 'Migration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imported the Barro educational attainment dataset\n",
    "education = pd.read_excel(r'Datasets/BL2013_F_v2.2.xls').drop(range(13))\n",
    "\n",
    "#Dropped Columns \n",
    "education = education.drop(columns = ['Unnamed: 4',\n",
    "                                      'Unnamed: 5',\n",
    "                                      'Unnamed: 6',\n",
    "                                      'Unnamed: 7',\n",
    "                                      'Unnamed: 9',\n",
    "                                      'Unnamed: 10',\n",
    "                                      'Unnamed: 11',\n",
    "                                      'Unnamed: 12',\n",
    "                                      'Unnamed: 13',\n",
    "                                      'Unnamed: 14',\n",
    "                                      'Unnamed: 15',\n",
    "                                      'Unnamed: 16'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropped the rows with the age ranges that I wasn't looking at\n",
    "education = education[(education[\"Unnamed: 2\"] == 15) & (education[\"Unnamed: 3\"] == 999) | (education[\"Unnamed: 3\"] == 19)].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created a for loop replacing the NaN country values with the appropriate country name \n",
    "for x in range(3796):\n",
    "    if isinstance(education.iloc[x,0], str): #Assigned a value to cname if the cell had a string value\n",
    "        cname = education.iloc[x,0]\n",
    "    else: \n",
    "        education.loc[x,'Unnamed: 0'] = cname #Replaced NaN with the country name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropped the rows with the years I wasn't looking at\n",
    "education = education[(education[\"Unnamed: 1\"] == 1960) | (education[\"Unnamed: 1\"] == 1985) | (education[\"Unnamed: 1\"] == 2000)]\n",
    "\n",
    "#Dropped the age range I wasn't look at\n",
    "education = education[(education[\"Unnamed: 2\"] == 15) & (education[\"Unnamed: 3\"] == 999)]\n",
    "\n",
    "#Dropped columns \n",
    "education = education.drop(columns = ['Unnamed: 2','Unnamed: 3'])\n",
    "\n",
    "#Replaced the years\n",
    "education = education.replace([1960, 1985, 2000], [1961, 1985, 2001]).reset_index(drop = True)\n",
    "\n",
    "#Renamed the columns\n",
    "education.columns = ['Temp_Country', 'Year', 'Education']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changed the datatypes \n",
    "IMD['Year'] = IMD['Year'].astype(int)\n",
    "frac_2001['Year'] = frac_2001['Year'].astype(int)\n",
    "frac_6185['Year'] = frac_6185['Year'].astype(int)\n",
    "frac_2001['ELF'] = frac_2001['ELF'].astype(float)\n",
    "frac_6185['ELF'] = frac_6185['ELF'].astype(float)\n",
    "rural['Year'] = rural['Year'].astype(int)\n",
    "urban['Year'] = urban['Year'].astype(int)\n",
    "GDP['Year'] = GDP['Year'].astype(int)\n",
    "migration['Year'] = migration['Year'].astype(int)\n",
    "cereal['Year'] = cereal['Year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Country-converter variable to restrict the set to only the official recognized UN members\n",
    "cc_UN = coco.CountryConverter(only_UNmember=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def un_only(dta): \n",
    "    '''\n",
    "    Function that takes a dataframe with countries and passes it through the country_converter package to remove any \n",
    "    non-UN countries from the dataframe. \n",
    "\n",
    "    Args:\n",
    "        dta (dataframe): dataframe with a column of countries \n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: frame containing scraped tables\n",
    "    '''\n",
    "    UN_list = dta.Temp_Country.tolist() #Created a new list from the Country column of the inputted dataset\n",
    "    UN = cc_UN.convert(UN_list, to = 'name_short') #Ran cc_UN on the dataset and saved that list as 'UN'\n",
    "    dta = dta.drop(columns = \"Temp_Country\") #Dropped the 'Temp_Country' column from the dataframe\n",
    "    dta[\"Country\"] = UN #Created a new column called 'Country' and set those values as 'UN'\n",
    "    dta = dta.set_index('Country') #Set the index as the 'Country' column\n",
    "    dta = dta.drop(index = \"not found\") #Set the index as the 'Country' column\n",
    "    dta = dta.reset_index() #Reset the index in order to remove the 'Country' column as the index\n",
    "    return dta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_output"
    ]
   },
   "outputs": [],
   "source": [
    "#Passed the datasets into 'un_only' and resaved the datasets\n",
    "IMD = un_only(IMD)\n",
    "frac_2001 = un_only(frac_2001)\n",
    "frac_6185 = un_only(frac_6185)\n",
    "rural = un_only(rural)\n",
    "urban = un_only(urban)\n",
    "migration = un_only(migration)\n",
    "GDP = un_only(GDP)\n",
    "education = un_only(education)\n",
    "cereal = un_only(cereal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing Rural, Urban, GDP, and Education into decimal values \n",
    "rural['Rural_%'] *= 0.01\n",
    "urban['Urban_%'] *= 0.01\n",
    "GDP['GDP'] *= 0.01\n",
    "education['Education'] *= 0.01\n",
    "\n",
    "#Converting IMD into a percentage\n",
    "IMD['Infant'] /= 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outer merged the globalization factors to the frac_2001 dataset \n",
    "a = pd.merge(IMD, frac_2001, on = ['Country', 'Year'], how = 'outer')\n",
    "b = pd.merge(a, rural, on = ['Country', 'Year'], how = 'outer')\n",
    "c = pd.merge(b, migration, on = ['Country', 'Year'], how = 'outer') \n",
    "d = pd.merge(c, GDP, on = ['Country', 'Year'], how = 'outer')\n",
    "e = pd.merge(d, urban, on = ['Country', 'Year'], how = 'outer')\n",
    "f = pd.merge(e, cereal, on = ['Country', 'Year'], how = 'outer')\n",
    "frac_2001 = pd.merge(f, education, on = ['Country', 'Year'], how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Included only 2001 data for frac_2001\n",
    "frac_2001 = frac_2001[frac_2001.Year == 2001].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outer merged the globalization factors to the frac_6185 dataset \n",
    "g = pd.merge(IMD, frac_6185, on = ['Country', 'Year'], how = 'outer')\n",
    "h = pd.merge(g, rural, on = ['Country', 'Year'], how = 'outer')\n",
    "i = pd.merge(h, migration, on = ['Country', 'Year'], how = 'outer') \n",
    "j = pd.merge(i, GDP, on = ['Country', 'Year'], how = 'outer')\n",
    "k = pd.merge(j, urban, on = ['Country', 'Year'], how = 'outer')\n",
    "l = pd.merge(k, cereal, on = ['Country', 'Year'], how = 'outer')\n",
    "frac_6185 = pd.merge(l, education, on = ['Country', 'Year'], how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Included only 1961 & 1985 data for frac_6185\n",
    "frac_6185 = frac_6185[frac_6185.Year != 2001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outer merged both fractionalization datasets \n",
    "frac = pd.merge(frac_2001, frac_6185, on = ['Country', 'Year', 'ELF', 'Rural_%', 'Urban_%', 'Infant', 'Migration', 'GDP', 'Cereal_Yield', 'Education'], how = 'outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving forward I think it’s really important to note here the biggest limitations of my research project and one that has affected my results: the lack of data when it comes to ELF measurements. Of the two ELF datasets that I was able to gather, the unedited datasources only left me with 581 observational data points. After running the datasets through the `Country Converter`[^23] package to a) standardize the country names across all datasets and b) include only UN member-nations; and dropping NA values I was only left with 295 observations to analyze.\n",
    "\n",
    "Originally, and within my video presentation, I created the following visualization, FIGURE 1, using the `Seaborn`[^24] and `Matplotlib` [^25] package to create a dual regression plot comparing the ELF trends against the trend of IMR over time. \n",
    "\n",
    "But this felt like a misleading graph as I had two different ELF datasets (one from 1961 and 1985, and another one from 2001) and I was unclear if the respective researchers used the same methodologies when calculating ELF.  This is particularly tricky aspect as ELF calculations entirely depends upon how the researcher qualifies ethic minorities within a country. Are White Americans ethnically and linguistically different than Black Americans? \n",
    "\n",
    "As such I did not feel comfortable merging and considering the dataset to be one and decide to focus my analysis into 2 parts: historic trends (ELF from 1961 and 1985) and “modern” predictions (ELF from 2001). \n",
    "\n",
    "[^23]: Stadler, K. (2017). The country converter coco - a Python package for converting country names between different classification schemes. The Journal of Open Source Software. doi: 10.21105/joss.00332\n",
    "\n",
    "[^24]: Michael Waskom et. al. (2020, September 8). mwaskom/seaborn: v0.11.0 (Sepetmber 2020) (Version v0.11.0). Zenodo. http://doi.org/10.5281/zenodo.4019146\n",
    "\n",
    "[^25]:  Thomas A Caswell, et. al. (2020, November 12). matplotlib/matplotlib: REL: v3.3.3 (Version v3.3.3). Zenodo. http://doi.org/10.5281/zenodo.4268928"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale = 2)\n",
    "fig, ax = plt.subplots(figsize=(16, 12))\n",
    "sns.regplot(x=\"Year\", y= \"ELF\", data = frac, color = 'red', ax = ax, scatter = False).set_title(\"ELF Trend v IMR Trend (Total)\")\n",
    "ax2 = ax.twinx()\n",
    "sns.regplot(x=\"Year\", y= \"Infant\", data = frac, color = 'green', ax = ax2, scatter = False);\n",
    "txt=\"Figure 1\"\n",
    "plt.figtext(0.5, -0.01, txt, wrap=True, horizontalalignment='center', fontsize=20)\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historic Trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first part of my analysis I am going to take a look at the 1961 and 1985 dataset and visually see if any of the globalization indexes bears a particularly strong correlation with ELF. The purpose of this section is to choose the strongest correlations visually to then include in our “modern” prediction. I will do this, predominately, by using the `Seaborn` package as well the `Plotnine`[^26] package. \n",
    "\n",
    "The first visualization is a simple correlation heatmap, FIGURE 2, showing which of the globalization factors has the strongest correlation to ELF. According to these results *Infant*, *Urban_%*, and *Rural_%* have the top 3 strongest correlations. Rather than outputting visualizations for all 3 variables, I will just focus on the *Infant* variable. \n",
    "\n",
    "FIGURE 3 shows a dual regression plot between *ELF* and *Infant*. At first glance there it seems to be a wholly uninformative graph seeing as *ELF* has minimal to no change. It almost appears to be a horizontal line but I think this learning makes sense as *ELF* should have more value on a national level rather than an international level. \n",
    "\n",
    "Another important thing to note is the spread of of the data *ELF* is incredibly spread out whereas *Infant* is fairly close to the predicted regression line. This means that regardless of the strength of the prediction it will likely be a weak predictor given that *ELF*, by itself, has such a wide spread.\n",
    "\n",
    "I’ve also included FIGURE 4 in order to explore the spread further. I wanted to know if the relationship between *ELF* and *Infant* changes based on the year; does the spread stay constant or does the relationship change? \n",
    "\n",
    "As we can see in FIGURE 4 4, there is a change in the relationship but a change we already knew about. The data squishes more horizontally rather than vertically meaning that only the *Infant* relationship changed rather than the *ELF* data on a country by country basis. \n",
    "\n",
    "Our historical analysis shows that even if a relationship exists, it’s bound to be a fairly weak one. \n",
    "\n",
    "[^26]: Hassan Kibirige, et. al. (2020, August 5). has2k1/plotnine: v0.7.1 (Version v0.7.1). Zenodo. http://doi.org/10.5281/zenodo.3973626"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created a variable that only selected the float datatype and calculated the respective correlation\n",
    "frac_corr = frac_6185.select_dtypes(include=['float64']).corr()\n",
    "\n",
    "#Plotted the \"frac_corr\" variable by creating a correlation heatmaps\n",
    "sns.set(font_scale = 1.5)\n",
    "plt.figure(figsize = (10,10));\n",
    "sns.heatmap(frac_corr, center=0, linewidths=.5, cmap = \"YlGnBu\", annot=True)#Used the 'annot' value to provide the exact correlation values\n",
    "plt.title ('Correlation Heatmap of ELF to Globalization Factors')\n",
    "txt=\"Figure 2\"\n",
    "plt.figtext(0.5, -0.01, txt, wrap=True, horizontalalignment='center', fontsize=17)\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale = 2)\n",
    "fig, ax = plt.subplots(figsize=(16, 12))\n",
    "sns.regplot(x=\"Year\", y= \"ELF\", data = frac_6185, color = 'red', ax = ax, scatter = False).set_title(\"ELF Trend v IMR Trend (1961, 1985)\")\n",
    "ax2 = ax.twinx()\n",
    "sns.regplot(x=\"Year\", y= \"Infant\", data = frac_6185, color = 'green', ax = ax2, scatter = False);\n",
    "txt=\"Figure 3\"\n",
    "plt.figtext(0.5, -0.01, txt, wrap=True, horizontalalignment='center', fontsize=20)\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ggplot(frac_6185, aes(x='Infant', y='ELF', color = \"Country\")) +\n",
    "geom_point(size = 3) +\n",
    "facet_wrap(['Year']) +\n",
    "labs(title=\"Figure 4: Spread of ELF to IMR (1961, 1985)\") +\n",
    "theme(legend_position = (0.5, -0.15), figure_size=(16, 12)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Modern\" Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for Missing Values\n",
    "miss.bar(frac_2001);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns\n",
    "frac_2001 = frac_2001.drop(columns = ['Country', 'Year', 'Migration', 'GDP', 'Migration', 'Education', 'Cereal_Yield'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping NA values \n",
    "NA_2001 = frac_2001[~frac_2001.isna().any(axis=1)]\n",
    "\n",
    "NA_2001.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into test and training data\n",
    "y = NA_2001['ELF']\n",
    "X = NA_2001.drop(columns=['ELF'])\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = .25, random_state = 111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotted the float variables\n",
    "d = train_X.select_dtypes(include=\"float\").melt()\n",
    "(\n",
    "    ggplot(d,aes(x=\"value\")) +\n",
    "    geom_histogram(bins = 25) +\n",
    "    facet_wrap(\"variable\",scales='free') +\n",
    "    labs(title=\"Figure 5: Spread of the Globalization Factors (2001)\") +\n",
    "    theme(figure_size=(10,3),\n",
    "          subplots_adjust={'wspace':0.5}, panel_spacing_y=0.2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the folds index to ensure comparable samples\n",
    "fold_generator = KFold(n_splits=10, shuffle=True,random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created an array of all possible options\n",
    "options = np.arange(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Passed a dictionary of all the tuning parameter values that we want to explore\n",
    "knn_tune_params = {'n_neighbors':options}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrapped the model method in the GridSearchCV() class\n",
    "tune_knn = GridSearchCV(KNN(),knn_tune_params,\n",
    "                        cv = fold_generator,\n",
    "                        scoring='neg_mean_squared_error',\n",
    "                        n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tunned KNN\n",
    "tune_knn.fit(train_X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best number of neighbors\n",
    "tune_knn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrapped the model method in the GridSearchCV() class\n",
    "tune_dt = GridSearchCV(DT(),{'max_depth':[i for i in range(1500)]},\n",
    "                        cv = fold_generator,\n",
    "                        scoring='neg_mean_squared_error',\n",
    "                        n_jobs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tunned the Decision Tree\n",
    "tune_dt.fit(train_X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best tree depth size\n",
    "tune_dt.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrapped the model method in the GridSearchCV() class\n",
    "rf_params = {'max_depth':[1],\n",
    "             'n_estimators':[i for i in range(50)],\n",
    "              'max_features': [1,2,3]} # Only have three total.\n",
    "tune_rf = GridSearchCV(RF(),rf_params,\n",
    "                        cv = fold_generator,\n",
    "                        scoring='neg_mean_squared_error',\n",
    "                        n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuned random forest\n",
    "tune_rf.fit(train_X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best parameters for random forest\n",
    "tune_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[('pre_process', pp.MinMaxScaler()),('model',None)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = [\n",
    "    # Linear Model\n",
    "    {'model' : [LM()]},\n",
    "    \n",
    "    # KNN with K tuning param\n",
    "    {'model' : [KNN()],\n",
    "     'model__n_neighbors':[1, 8, 10, 25, 48, 75]},\n",
    "    \n",
    "    # Decision Tree with the Max Depth Param\n",
    "    {'model': [DT(random_state=1234)],\n",
    "     'model__max_depth':[1,2,3,4]},\n",
    "    \n",
    "    # Random forest with the N Estimators tuning param\n",
    "    {'model' : [RF(random_state=1234)],\n",
    "    'model__max_depth':[1,2,3,4],\n",
    "    'model__n_estimators':[i for i in range(50)],\n",
    "    'model__max_features':[1,2,3]}\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GridSearchCV(pipe, search_space, \n",
    "                      cv = fold_generator,\n",
    "                      scoring='neg_mean_squared_error',\n",
    "                      n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "search.fit(train_X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best score\n",
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best parameters\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labelled the best model\n",
    "rf_mod = search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ran predict on the test data to use the best model\n",
    "pred_y = search.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi = permutation_importance(rf_mod,train_X,train_y,n_repeats=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize as a data frame \n",
    "vi_dat = pd.DataFrame(dict(variable=train_X.columns,\n",
    "                           vi = vi['importances_mean'],\n",
    "                           std = vi['importances_std']))\n",
    "\n",
    "# Generate intervals\n",
    "vi_dat['low'] = vi_dat['vi'] - 2*vi_dat['std']\n",
    "vi_dat['high'] = vi_dat['vi'] + 2*vi_dat['std']\n",
    "\n",
    "# But in order from most to least important\n",
    "vi_dat = vi_dat.sort_values(by=\"vi\",ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "vi_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determined the MSE of the prediction\n",
    "m.mean_squared_error(test_y,pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotted the prediction against the actual data\n",
    "(\n",
    "    ggplot(pd.DataFrame(dict(pred=pred_y,truth=test_y)),\n",
    "          aes(x='pred',y=\"truth\")) +\n",
    "    geom_point(alpha=.75) +\n",
    "    geom_abline(linetype=\"dashed\",color=\"darkred\",size=1) +\n",
    "    theme_bw() +\n",
    "    labs(title=\"Figure 6: Predictions Against The Actual Values\") +\n",
    "    theme(figure_size=(12,7))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Sahithi N Adari"
   }
  ],
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "title": "Linguistic Fractionalization and Globalization"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
